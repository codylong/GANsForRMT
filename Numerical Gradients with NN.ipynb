{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Gradients with Neural Nets\n",
    "\n",
    "As before, first we will get things rocking and rolling with Cody's old data at h11=15.\n",
    "\n",
    "Our goal will be to have a simple FFNN construct the random matrix for us. This should require only a few modifications of the old code. I'll track the changes, for simplicity:\n",
    "\n",
    "- change random matrix to be the output of a neural network\n",
    "- change sample to use random_matrix() (before was random_wishart()), make only numdraws and nn dep\n",
    "- added AAtranspose method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wasserstein_distance\n",
    "\n",
    "First we must adapt scipy's wasserstein_distance function utilize torch instead of numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from torchsearchsorted import searchsorted\n",
    "import numpy as np\n",
    "from scipy.stats import wasserstein_distance\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "\n",
    "# set up test nn\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else: device = torch.device('cpu')\n",
    "    \n",
    "print device    \n",
    "    \n",
    "test_nn = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1,1000),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(1000,225)\n",
    ").to(device)\n",
    "\n",
    "\n",
    "real_eigs_cody = np.loadtxt(\"ppo2/h11=15.txt\")\n",
    "real_eigs_cody = torch.Tensor(real_eigs_cody).to(device)\n",
    "\n",
    "def remove_nan(t):\n",
    "    nan = torch.isnan(t[0])\n",
    "    o = torch.tensor([])\n",
    "    for idx,p in enumerate(nan):\n",
    "        #print p, o\n",
    "        if p == 0:\n",
    "            o = torch.cat((o,torch.tensor([t[0][idx]])))\n",
    "        else:\n",
    "            o = torch.cat((o,torch.tensor([0.0])))\n",
    "    return torch.tensor([list(o)])\n",
    "\n",
    "def _cdf_distance(p, u_values, v_values):\n",
    "    #print \"input\", u_values, v_values\n",
    "    #print '\\nsorter'\n",
    "    u_sorter = torch.argsort(u_values).to(device)\n",
    "    v_sorter = torch.argsort(v_values).to(device)\n",
    "\n",
    "    #print 'allvals'\n",
    "    all_values = torch.cat((u_values, v_values))\n",
    "    all_values, _ = all_values.sort()\n",
    "    #print'av',all_values\n",
    "\n",
    "#     # Compute the differences between pairs of successive values of u and v.\n",
    "    deltas = all_values[1:] - all_values[:-1]  # replaces call to np.diff\n",
    "#     print 'searchsorted'\n",
    "#     u_cdf_indices = searchsorted(torch.tensor([list([u_values[u_sorter]][0])]), torch.tensor([list([all_values[:-1]][0])])).to(device)\n",
    "#     v_cdf_indices = searchsorted(torch.tensor([list([v_values[v_sorter]][0])]), torch.tensor([list([all_values[:-1]][0])])).to(device)\n",
    "#     #print 'ucdfind', u_cdf_indices\n",
    "\n",
    "    u1,u2 = remove_nan(torch.tensor([list([u_values[u_sorter]][0])])), remove_nan(torch.tensor([list([all_values[:-1]][0])]))\n",
    "    u_cdf_indices = searchsorted(u1,u2).to(device)\n",
    "    v1, v2 = remove_nan(torch.tensor([list([v_values[v_sorter]][0])])),remove_nan(torch.tensor([list([all_values[:-1]][0])]))\n",
    "    v_cdf_indices = searchsorted(v1,v2).to(device)\n",
    "\n",
    "    #print 'cdf'\n",
    "    # Calculate the CDFs of u and v using their weights, if specified.\n",
    "    u_cdf = u_cdf_indices / u_values.shape[0]\n",
    "    v_cdf = v_cdf_indices / v_values.shape[0]\n",
    "\n",
    "    #print 'uv', u_cdf, v_cdf\n",
    "\n",
    "    #print 'check1', deltas\n",
    "    #print 'sum and multiply'\n",
    "    \n",
    "    return torch.sum(torch.abs(u_cdf-v_cdf)*deltas)\n",
    "    return np.sum(np.multiply(np.abs(u_cdf - v_cdf), deltas))\n",
    "    #print 'return from loss'\n",
    "    return torch.pow(torch.sum(torch.pow(torch.abs(u_cdf - v_cdf), p) * deltas), 1 / p)\n",
    "\n",
    "\n",
    "def wasserstein_distance(u, v):\n",
    "    return _cdf_distance(1, u, v)\n",
    "\n",
    "\n",
    "print wasserstein_distance(real_eigs_cody,real_eigs_cody)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "def initial_vars(nmod):\n",
    "    return [1./np.float(nmod)**(1./2.) for i in range(nmod + 2)]\n",
    "\n",
    "def AAtranspose(A): # computes AAtrans for a matrix A\n",
    "    return torch.mm(A,torch.transpose(A,0,1))\n",
    "\n",
    "def random_matrix(nn=test_nn): # pos def metric\n",
    "    mat = nn(torch.rand(1).to(device)).reshape(15,15)\n",
    "    return AAtranspose(mat)\n",
    "\n",
    "def sample(numdraws,nn=test_nn):\n",
    "    evals = torch.tensor([]).to(device)\n",
    "    #print 'numdraws'\n",
    "    for num in range(numdraws):\n",
    "        #print num\n",
    "        evals = torch.cat((evals,torch.symeig(random_matrix(nn=nn),eigenvectors=True)[0]))\n",
    "    #    print 'end eval'\n",
    "    return evals\n",
    "\n",
    "def wasserstein_loss(numdraws=300, numspectra=1, real_eigs = real_eigs_cody,nn=test_nn): # h11 = len(vars)-2\n",
    "    return wasserstein_distance(torch.log10(sample(numdraws,nn=nn)),real_eigs)\n",
    "    #return torch.mean([wasserstein_distance(sample(numdraws,nn=nn).cpu().detach().numpy(),real_eigs.cpu().detach().numpy()) for i in range(numspectra)])\n",
    "\n",
    "def gradient_descent(num_epochs=1000, lr = .001, logplot = True, real_eigs = real_eigs_cody, opt = torch.optim.Adam, hidden_dim = 1500):\n",
    "        \n",
    "    loss, numsteps = 1e6, 0\n",
    "    best = loss\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else: device = torch.device('cpu')\n",
    "        \n",
    "    train_nn = torch.nn.Sequential(\n",
    "        torch.nn.Linear(1,hidden_dim),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(.05),\n",
    "        torch.nn.Linear(hidden_dim,hidden_dim),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(.05),\n",
    "        torch.nn.Linear(hidden_dim,hidden_dim),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(.05),\n",
    "        torch.nn.Linear(hidden_dim,hidden_dim),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(.05),\n",
    "        torch.nn.Linear(hidden_dim,hidden_dim),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(.05),\n",
    "        torch.nn.Linear(hidden_dim,hidden_dim),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(.05),\n",
    "        torch.nn.Linear(hidden_dim,hidden_dim),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(.05),\n",
    "        torch.nn.Linear(hidden_dim,hidden_dim),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(.05),\n",
    "        torch.nn.Linear(hidden_dim,hidden_dim),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(.05),\n",
    "        torch.nn.Linear(hidden_dim,hidden_dim),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(.05),\n",
    "        torch.nn.Linear(hidden_dim,hidden_dim),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(.05),\n",
    "        torch.nn.Linear(hidden_dim,225)\n",
    "    ).to(device)\n",
    "    optimizer = opt(train_nn.parameters(), lr=lr)    \n",
    "        \n",
    "    # train\n",
    "    loss, numsteps = 1e6, 0\n",
    "    best = loss\n",
    "    lastlossplotted = loss\n",
    "    \n",
    "    runningloss = 0\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        #print 'compute loss'\n",
    "        loss = wasserstein_loss(nn=train_nn, real_eigs = real_eigs)\n",
    "        #print \"epoch\", epoch, \"loss\", loss\n",
    "        #print 'backprop'\n",
    "        loss.backward()\n",
    "        #print 'step'\n",
    "        optimizer.step()\n",
    "        #print 'end step'\n",
    "    \n",
    "        runningloss += loss\n",
    "    \n",
    "        # prints and trivial  updates\n",
    "        if loss < best:\n",
    "            best = loss\n",
    "            print \"new best:\", loss\n",
    "            if lastlossplotted - best > .1:\n",
    "                lastlossplotted = best\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    if logplot:\n",
    "                        sns.distplot([torch.log10(k) for k in real_eigs.cpu()], kde=False, label='10', norm_hist = True)\n",
    "                        samp = [float(torch.log10(k)) for k in sample(1000).cpu()]\n",
    "                        #print samp\n",
    "                        df = pd.DataFrame(samp)\n",
    "                        #print [torch.log10(k) for k in sample(300).cpu()]\n",
    "                        sns.distplot(df.dropna(), kde=False, label='10', norm_hist = True)\n",
    "                    else:\n",
    "                        sns.distplot([k for k in real_eigs.cpu()], kde=False, label='10', norm_hist = True)\n",
    "                        samp = [float(k) for k in sample(300).cpu()]\n",
    "                        df = pd.DataFrame(samp)\n",
    "                        sns.distplot(df.dropna(), kde=False, label='10', norm_hist = True)\n",
    "                \n",
    "                plt.show()\n",
    "        \n",
    "        if numsteps % 10 == 0: \n",
    "            print \"steps: {} \\t avg loss last 10: {}\".format(numsteps,runningloss/10), datetime.datetime.now()\n",
    "            runningloss = 0\n",
    "        numsteps += 1\n",
    "\n",
    "    return train_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps: 0 \t avg loss last 10: nan 2019-10-14 14:25:24.948367\n",
      "new best: tensor(11.3604, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best: tensor(11.3603, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 10 \t avg loss last 10: nan 2019-10-14 14:25:45.339508\n",
      "new best: tensor(11.3543, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "new best: tensor(11.3503, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 20 \t avg loss last 10: nan 2019-10-14 14:26:04.451892\n",
      "steps: 30 \t avg loss last 10: nan 2019-10-14 14:26:23.356416\n",
      "steps: 40 \t avg loss last 10: nan 2019-10-14 14:26:42.151518\n",
      "steps: 50 \t avg loss last 10: nan 2019-10-14 14:27:00.941035\n",
      "steps: 60 \t avg loss last 10: nan 2019-10-14 14:27:19.735527\n",
      "steps: 70 \t avg loss last 10: nan 2019-10-14 14:27:38.521702\n",
      "new best: tensor(11.3489, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 80 \t avg loss last 10: nan 2019-10-14 14:27:57.288587\n",
      "steps: 90 \t avg loss last 10: nan 2019-10-14 14:28:16.445803\n",
      "steps: 100 \t avg loss last 10: nan 2019-10-14 14:28:35.458642\n",
      "steps: 110 \t avg loss last 10: nan 2019-10-14 14:28:54.650577\n",
      "new best: tensor(11.3452, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "new best: tensor(11.3442, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "new best: tensor(11.3394, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 120 \t avg loss last 10: nan 2019-10-14 14:29:14.223661\n",
      "new best: tensor(11.3382, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 130 \t avg loss last 10: nan 2019-10-14 14:29:34.324024\n",
      "new best: tensor(11.3267, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 140 \t avg loss last 10: nan 2019-10-14 14:29:54.177483\n",
      "new best: tensor(11.3262, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 150 \t avg loss last 10: nan 2019-10-14 14:30:14.493165\n",
      "steps: 160 \t avg loss last 10: nan 2019-10-14 14:30:34.782099\n",
      "new best: tensor(11.3255, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 170 \t avg loss last 10: nan 2019-10-14 14:30:54.503506\n",
      "steps: 180 \t avg loss last 10: nan 2019-10-14 14:31:14.367664\n",
      "new best: tensor(11.3252, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 190 \t avg loss last 10: nan 2019-10-14 14:31:34.349480\n",
      "new best: tensor(11.3238, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 200 \t avg loss last 10: nan 2019-10-14 14:31:54.335204\n",
      "new best: tensor(11.3238, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "new best: tensor(11.3232, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 210 \t avg loss last 10: nan 2019-10-14 14:32:13.426658\n",
      "new best: tensor(11.3222, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 220 \t avg loss last 10: nan 2019-10-14 14:32:32.986291\n",
      "steps: 230 \t avg loss last 10: nan 2019-10-14 14:32:53.209877\n",
      "new best: tensor(11.3221, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "new best: tensor(11.3219, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "new best: tensor(11.3184, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "new best: tensor(11.3120, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 240 \t avg loss last 10: nan 2019-10-14 14:33:12.821376\n",
      "new best: tensor(11.3089, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "new best: tensor(11.3045, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 250 \t avg loss last 10: nan 2019-10-14 14:33:32.271785\n",
      "new best: tensor(11.3035, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 260 \t avg loss last 10: nan 2019-10-14 14:33:52.192909\n",
      "new best: tensor(11.3010, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "new best: tensor(11.2981, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 270 \t avg loss last 10: nan 2019-10-14 14:34:11.885146\n",
      "new best: tensor(11.2945, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 280 \t avg loss last 10: nan 2019-10-14 14:34:31.836013\n",
      "steps: 290 \t avg loss last 10: nan 2019-10-14 14:34:51.034315\n",
      "steps: 300 \t avg loss last 10: 11.3020248413 2019-10-14 14:35:10.001737\n",
      "steps: 310 \t avg loss last 10: nan 2019-10-14 14:35:30.470873\n",
      "steps: 320 \t avg loss last 10: nan 2019-10-14 14:35:50.234964\n",
      "steps: 330 \t avg loss last 10: nan 2019-10-14 14:36:09.849174\n",
      "steps: 340 \t avg loss last 10: nan 2019-10-14 14:36:30.118077\n",
      "steps: 350 \t avg loss last 10: nan 2019-10-14 14:36:50.607371\n",
      "steps: 360 \t avg loss last 10: nan 2019-10-14 14:37:10.423888\n",
      "new best: tensor(11.2939, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 370 \t avg loss last 10: nan 2019-10-14 14:37:31.359631\n",
      "steps: 380 \t avg loss last 10: nan 2019-10-14 14:37:51.781947\n",
      "steps: 390 \t avg loss last 10: nan 2019-10-14 14:38:11.828885\n",
      "steps: 400 \t avg loss last 10: nan 2019-10-14 14:38:32.586149\n",
      "new best: tensor(11.2926, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 410 \t avg loss last 10: nan 2019-10-14 14:38:53.093765\n",
      "steps: 420 \t avg loss last 10: nan 2019-10-14 14:39:13.951152\n",
      "steps: 430 \t avg loss last 10: nan 2019-10-14 14:39:34.470759\n",
      "steps: 440 \t avg loss last 10: nan 2019-10-14 14:39:55.874496\n",
      "steps: 450 \t avg loss last 10: nan 2019-10-14 14:40:17.203560\n",
      "steps: 460 \t avg loss last 10: nan 2019-10-14 14:40:37.866967\n",
      "steps: 470 \t avg loss last 10: nan 2019-10-14 14:40:58.589369\n",
      "steps: 480 \t avg loss last 10: nan 2019-10-14 14:41:17.987298\n",
      "steps: 490 \t avg loss last 10: nan 2019-10-14 14:41:38.775423\n",
      "steps: 500 \t avg loss last 10: nan 2019-10-14 14:41:59.385726\n",
      "new best: tensor(11.2878, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "new best: tensor(11.2862, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "new best: tensor(11.2795, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "new best: tensor(11.2786, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "new best: tensor(11.2778, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 510 \t avg loss last 10: nan 2019-10-14 14:42:20.195648\n",
      "new best: tensor(11.2753, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "new best: tensor(11.2722, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "new best: tensor(11.2697, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "new best: tensor(11.2684, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "new best: tensor(11.2682, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "new best: tensor(11.2669, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 520 \t avg loss last 10: 11.2717561722 2019-10-14 14:42:40.394903\n",
      "new best: tensor(11.2662, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "new best: tensor(11.2636, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 530 \t avg loss last 10: nan 2019-10-14 14:43:01.801672\n",
      "steps: 540 \t avg loss last 10: 11.2660360336 2019-10-14 14:43:22.440949\n",
      "new best: tensor(11.2628, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "new best: tensor(11.2621, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 550 \t avg loss last 10: nan 2019-10-14 14:43:42.249259\n",
      "steps: 560 \t avg loss last 10: 11.2659692764 2019-10-14 14:44:02.143862\n",
      "steps: 570 \t avg loss last 10: nan 2019-10-14 14:44:22.015917\n",
      "steps: 580 \t avg loss last 10: nan 2019-10-14 14:44:42.876361\n",
      "steps: 590 \t avg loss last 10: 11.2653274536 2019-10-14 14:45:03.841718\n",
      "steps: 600 \t avg loss last 10: 11.2657728195 2019-10-14 14:45:24.343519\n",
      "steps: 610 \t avg loss last 10: 11.2662467957 2019-10-14 14:45:44.609917\n",
      "steps: 620 \t avg loss last 10: nan 2019-10-14 14:46:04.112042\n",
      "new best: tensor(11.2620, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 630 \t avg loss last 10: 11.2652082443 2019-10-14 14:46:24.034292\n",
      "new best: tensor(11.2614, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 640 \t avg loss last 10: nan 2019-10-14 14:46:42.955926\n",
      "steps: 650 \t avg loss last 10: 11.265045166 2019-10-14 14:47:01.964836\n",
      "steps: 660 \t avg loss last 10: 11.2655553818 2019-10-14 14:47:21.282242\n",
      "new best: tensor(11.2613, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 670 \t avg loss last 10: nan 2019-10-14 14:47:41.096675\n",
      "steps: 680 \t avg loss last 10: 11.2655601501 2019-10-14 14:48:00.133078\n",
      "steps: 690 \t avg loss last 10: 11.2646160126 2019-10-14 14:48:19.861363\n",
      "steps: 700 \t avg loss last 10: nan 2019-10-14 14:48:39.077653\n",
      "steps: 710 \t avg loss last 10: 11.264752388 2019-10-14 14:48:58.995633\n",
      "steps: 720 \t avg loss last 10: nan 2019-10-14 14:49:18.413167\n",
      "steps: 730 \t avg loss last 10: 11.2644948959 2019-10-14 14:49:37.861656\n",
      "new best: tensor(11.2608, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 740 \t avg loss last 10: 11.2645397186 2019-10-14 14:49:57.818151\n",
      "steps: 750 \t avg loss last 10: 11.2635917664 2019-10-14 14:50:17.389005\n",
      "steps: 760 \t avg loss last 10: 11.2634735107 2019-10-14 14:50:36.932566\n",
      "steps: 770 \t avg loss last 10: 11.2636995316 2019-10-14 14:50:56.908478\n",
      "steps: 780 \t avg loss last 10: 11.2629785538 2019-10-14 14:51:17.199941\n",
      "steps: 790 \t avg loss last 10: 11.2636785507 2019-10-14 14:51:37.011539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best: tensor(11.2600, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps: 800 \t avg loss last 10: 11.2635240555 2019-10-14 14:51:58.626400\n",
      "steps: 810 \t avg loss last 10: 11.263633728 2019-10-14 14:52:18.477731\n",
      "steps: 820 \t avg loss last 10: 11.2629299164 2019-10-14 14:52:37.847616\n",
      "steps: 830 \t avg loss last 10: 11.2640171051 2019-10-14 14:52:57.418468\n",
      "new best: tensor(11.2599, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 840 \t avg loss last 10: 11.2629213333 2019-10-14 14:53:16.706357\n",
      "steps: 850 \t avg loss last 10: nan 2019-10-14 14:53:35.930277\n",
      "steps: 860 \t avg loss last 10: nan 2019-10-14 14:53:55.953083\n",
      "new best: tensor(11.2590, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 870 \t avg loss last 10: nan 2019-10-14 14:54:15.492495\n",
      "steps: 880 \t avg loss last 10: 11.2623853683 2019-10-14 14:54:35.083390\n",
      "steps: 890 \t avg loss last 10: 11.2628755569 2019-10-14 14:54:54.231710\n",
      "new best: tensor(11.2578, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 900 \t avg loss last 10: nan 2019-10-14 14:55:13.775127\n",
      "steps: 910 \t avg loss last 10: 11.2627296448 2019-10-14 14:55:33.194210\n",
      "steps: 920 \t avg loss last 10: nan 2019-10-14 14:55:53.241698\n",
      "steps: 930 \t avg loss last 10: 11.261136055 2019-10-14 14:56:13.211252\n",
      "steps: 940 \t avg loss last 10: nan 2019-10-14 14:56:32.821387\n",
      "new best: tensor(11.2576, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "steps: 950 \t avg loss last 10: 11.2612075806 2019-10-14 14:56:52.230132\n",
      "steps: 960 \t avg loss last 10: 11.2611789703 2019-10-14 14:57:12.098086\n",
      "steps: 970 \t avg loss last 10: 11.2601108551 2019-10-14 14:57:33.195079\n",
      "steps: 980 \t avg loss last 10: 11.2588481903 2019-10-14 14:57:52.911244\n"
     ]
    }
   ],
   "source": [
    "out = gradient_descent(num_epochs = 1000000,lr=.000001,hidden_dim = 1000)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = gradient_descent(num_epochs = 1000000,lr=.00001)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent(num_epochs = 5000,lr=.001)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent(initial_vars(10), gradwidth=.01, lr=.0001, logplot = False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systematic Calabi-Yau Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CY_eigs = {\n",
    "          10: np.loadtxt(\"KS4/h11_10_eigs.txt\"),\n",
    "          20: np.loadtxt(\"KS4/h11_20_eigs.txt\"),\n",
    "          30: np.loadtxt(\"KS4/h11_30_eigs.txt\"),\n",
    "          40: np.loadtxt(\"KS4/h11_40_eigs.txt\"),\n",
    "          50: torch.tensor(np.loadtxt(\"KS4/h11_50_eigs.txt\")).to(device)\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient_descent(initial_vars(10), num_iter=10000, gradwidth=.01, lr=.0001, real_eigs = CY_eigs[10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent(num_epochs=5000, lr=.001, real_eigs = CY_eigs[50]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.loadtxt(\"KS4/h11_50_eigs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
